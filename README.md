# awesome-deep-nlp



## IR

1. **DSSM**   
Learning deep structured semantic models for web search using clickthrough data, cikm2013 [Paper](http://www.msr-waypoint.net/pubs/198202/cikm2013_DSSM_fullversion.pdf) [code1](https://github.com/mranahmd/dssm-wemb-theano) [code2](https://github.com/outstandingcandy/dssm)
2. **CDSSM**  
3. **CLSM**   
A latent semantic model with convolutional-pooling structure for information retrieval, cikm2014
4. **DCNN: Dynamic convolutional neural network**  
a convolutional neural network for modeling sentences, acl2014
convolutional neural network architectures for matching natural language sentences, nips2014, noah
3. **BRAE: bilingually-constrained recursive auto-encoders**  
bilingually-constrained phrase embeddings for machine translation, acl2014, long paper
6. **LSTM-RNN**  
Deep sentence embedding using lstm networks: analysis and application to information retrieval, 201602
7. **SkipThought**
8. **Doc2Vec**
9. **Bidirectional LSTM-RNN**

## Knowledge Graph

## awesome-deep-machine-learning
# Kernel

1.
Bayesian Nonparametric Kernel-Learning, NIPS2015  
针对问题：1.kernel需要predine，对quality of the finite sample estimator 有影响。-> data-driven kernel function. 2. N*N Gram矩阵需要计算，无法应用于大规模数据集。  

Random features have been recently
shown to be an effective way to scale
kernel methods to large datasets.
Roughly speaking, random feature
techniques like random kitchen sinks
(RKS) [18] work as follows.  Bochners theorem states that a continuous shift-invariant kernel K(x, y) = k(x − y) is a
positive definite function if and only if k(t) is the Fourier transform of a non-negative measure
ρ(ω).

